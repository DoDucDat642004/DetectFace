{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom glob import glob\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom torch import amp\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom tqdm import tqdm\n\n# ===== CHUẨN HOÁ NHÃN (mapping cho AffectNet hoặc FER+) =====\nEMOTION_MAP = {\n    \"anger\": \"angry\",\n    \"disgust\": \"angry\",      # gộp disgust vào angry\n    \"contempt\": \"angry\",     # gộp contempt vào angry\n    \"fear\": \"fear\",\n    \"happiness\": \"happy\",\n    \"sadness\": \"sad\",\n    \"surprise\": \"surprise\",\n    \"neutral\": \"neutral\"\n}\n\n# ===== CÁC CLASS HỢP LỆ (6-class gộp phù hợp nhất) =====\nVALID_CLASSES = [\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n\n# ===== LÀM SẠCH NHÃN =====\ndef clean_labels(df):\n    \"\"\"\n    Chuẩn hóa & gộp nhãn cho FER+ và AffectNet.\n    - lowercase, bỏ khoảng trắng, sửa chính tả\n    - gộp disgust & contempt vào angry\n    \"\"\"\n    df['label'] = df['label'].str.lower().str.strip()\n    df['label'] = df['label'].replace({\n        'suprise': 'surprise',    # fix chính tả\n        'happiness': 'happy',\n        'sadness': 'sad',\n        'anger': 'angry',\n        'disgust': 'angry',\n        'contempt': 'angry'\n    })\n    df = df[df['label'].isin(VALID_CLASSES)]\n    return df.reset_index(drop=True)\n\n\n# ===== LOAD DATASET THỐNG NHẤT =====\ndef load_dataset_unified(base_dir, exts=(\"jpg\", \"jpeg\", \"png\"), validate=True):\n    \"\"\"\n    Hỗ trợ 2 cấu trúc:\n    - base_dir/train/label/*.jpg\n    - base_dir/test/label/*.jpg\n    - base_dir/label/*.jpg\n    \"\"\"\n    rows = []\n    has_subset = any(name in os.listdir(base_dir) for name in [\"train\", \"test\"])\n    \n    if has_subset:\n        for subset in [\"train\", \"test\"]:\n            subset_dir = os.path.join(base_dir, subset)\n            if not os.path.isdir(subset_dir):\n                continue\n            for label in os.listdir(subset_dir):\n                label_dir = os.path.join(subset_dir, label)\n                if not os.path.isdir(label_dir):\n                    continue\n                for ext in exts:\n                    for path in glob(os.path.join(label_dir, f\"*.{ext}\")):\n                        rows.append({\"image_path\": path, \"label\": label, \"subset\": subset})\n    else:\n        for label in os.listdir(base_dir):\n            label_dir = os.path.join(base_dir, label)\n            if not os.path.isdir(label_dir):\n                continue\n            for ext in exts:\n                for path in glob(os.path.join(label_dir, f\"*.{ext}\")):\n                    rows.append({\"image_path\": path, \"label\": label})\n    \n    df = pd.DataFrame(rows)\n    \n    return df\n\n# ===== MERGE MULTIPLE DATASETS =====\ndef merge_datasets(dataset_configs, test_size=0.15, random_state=42, use_oversample=False):\n    \"\"\"\n    Hợp nhất nhiều dataset (FER+, AffectNet, ...) và chia train/val.\n\n    Args:\n        dataset_configs (list): danh sách dict cấu hình từng dataset\n        test_size (float): tỉ lệ validation\n        random_state (int): seed cố định để tái lập\n        use_oversample (bool): \n            True  -> tạo dữ liệu ảo cân bằng giữa các lớp (oversampling)\n            False -> giữ nguyên phân bố, dùng sample weight khi train\n    \"\"\"\n    all_data = []\n    \n    # --- Load và làm sạch từng dataset ---\n    for config in dataset_configs:\n        print(f\"\\nLoading {config['path']}...\")\n        df = load_dataset_unified(config[\"path\"], validate=config.get(\"validate\", True))\n        if df.empty:\n            print(f\"No data found in {config['path']}\")\n            continue\n\n        # Chuẩn hóa & mapping nhãn\n        df['label'] = df['label'].str.lower().str.strip()\n        if \"label_map\" in config:\n            df['label'] = df['label'].map(config[\"label_map\"]).fillna(df['label'])\n        df = clean_labels(df)\n\n        # Giữ lại subset train nếu có\n        if \"subset\" in df.columns and config.get(\"has_subset\", False):\n            df = df[df[\"subset\"] == \"train\"].drop(\"subset\", axis=1)\n        \n        print(f\"Loaded {len(df)} clean samples from {config['path']}\")\n        all_data.append(df)\n    \n    # --- Merge toàn bộ ---\n    df_merged = pd.concat(all_data, ignore_index=True)\n    df_merged = df_merged.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    print(\"\\nClass distribution (before balancing):\")\n    print(df_merged['label'].value_counts().sort_index())\n\n    # --- Chia train/val ---\n    train_df, val_df = train_test_split(\n        df_merged,\n        test_size=test_size,\n        stratify=df_merged['label'],\n        random_state=random_state\n    )\n\n    # --- Nếu không oversample ---\n    if not use_oversample:\n        train_df = train_df.copy()\n        train_df[\"is_augmented\"] = False\n        print(\"\\nOversampling disabled (using sample weights instead).\")\n        print(f\"Train size: {len(train_df)} | Val size: {len(val_df)}\")\n        print(\"\\nTrain class distribution:\")\n        print(train_df['label'].value_counts().sort_index())\n        return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\n    # --- Nếu bật oversampling ---\n    print(\"\\nPerforming oversampling to balance classes...\")\n    max_count = train_df['label'].value_counts().max()\n    print(f\"Largest class has {max_count} samples\")\n\n    balanced_parts = []\n    for label, group in train_df.groupby('label'):\n        if len(group) < max_count:\n            repeat_factor = int(np.ceil(max_count / len(group)))\n            augmented_group = pd.concat([group] * repeat_factor, ignore_index=True)\n            augmented_group = augmented_group.sample(max_count, random_state=random_state)\n            augmented_group[\"is_augmented\"] = False\n            augmented_group.iloc[:len(group) * (repeat_factor - 1),\n                                 augmented_group.columns.get_loc(\"is_augmented\")] = True\n            balanced_parts.append(augmented_group)\n        else:\n            group[\"is_augmented\"] = False\n            balanced_parts.append(group)\n\n    train_balanced = pd.concat(balanced_parts, ignore_index=True)\n    train_balanced = train_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    print(\"\\nTrain class distribution (after oversampling):\")\n    print(train_balanced['label'].value_counts().sort_index())\n    print(f\"\\nFinal Train size: {len(train_balanced)} | Val size: {len(val_df)}\")\n\n    return train_balanced.reset_index(drop=True), val_df.reset_index(drop=True)\n\n# ===== TRANSFORMS =====\nfrom torchvision import transforms\n\ndef get_transforms(img_size=112, use_pretrained=True):\n    \"\"\"\n    Chuẩn hóa transform cho FERPlus + AffectNet.\n    - use_pretrained=True: dành cho backbone pretrained (RGB, ImageNet mean/std)\n    - use_pretrained=False: dành cho model custom grayscale\n    \"\"\"\n    if use_pretrained:\n        # --- Dành cho pretrained ResNet (RGB, ImageNet mean/std) ---\n        train_t = transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),  # FERPlus → RGB\n            transforms.Resize((img_size + 8, img_size + 8)),\n            transforms.RandomResizedCrop(img_size, scale=(0.85, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(\n                brightness=0.2, contrast=0.2, saturation=0.1, hue=0.02\n            ),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(\n                p=0.25, scale=(0.02, 0.15), value=\"random\"\n            ),\n        ])\n\n        val_t = transforms.Compose([\n            transforms.Grayscale(num_output_channels=3),\n            transforms.Resize((img_size, img_size)),\n            transforms.CenterCrop(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    else:\n        # --- Cho model custom (1 kênh grayscale, không pretrained) ---\n        train_t = transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize((img_size + 8, img_size + 8)),\n            transforms.RandomResizedCrop(img_size, scale=(0.85, 1.0)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5]),\n            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n        ])\n\n        val_t = transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize((img_size, img_size)),\n            transforms.CenterCrop(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5]),\n        ])\n\n    return train_t, val_t\n\n\n\n# ===== DATASET CLASS =====\nclass EmotionDataset(Dataset):\n    def __init__(self, df, transform=None, preload=False):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.preload = preload\n        \n        # Label encoding (consistent order theo VALID_CLASSES)\n        labels = VALID_CLASSES\n        self.label_to_idx = {lbl: i for i, lbl in enumerate(labels)}\n        self.idx_to_label = {i: lbl for lbl, i in self.label_to_idx.items()}\n        self.num_classes = len(labels)\n        \n        # Preload nếu cần\n        self.images = None\n        if preload:\n            print(\"Preloading images to RAM...\")\n            self.images = [Image.open(p).convert('RGB') for p in tqdm(self.df['image_path'])]\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(row['image_path']).convert('RGB')\n        \n        transform = self.transform\n    \n        img = transform(img)\n    \n        label_str = row['label']\n        label = self.label_to_idx[label_str]\n        return img, torch.tensor(label, dtype=torch.long)\n\n    \n    def get_class_weights(self):\n        labels = [self.label_to_idx[lbl] for lbl in self.df['label']]\n        class_counts = Counter(labels)\n        total = len(labels)\n        weights = {cls: total / count for cls, count in class_counts.items()}\n        return torch.tensor([weights[i] for i in range(self.num_classes)], dtype=torch.float)\n    \n    def get_sampler(self):\n        labels = [self.label_to_idx[lbl] for lbl in self.df['label']]\n        class_counts = Counter(labels)\n        weights = [1.0 / class_counts[lbl] for lbl in labels]\n        return WeightedRandomSampler(weights, len(weights), replacement=True)\n\n# ===== MAIN =====\nif __name__ == \"__main__\":\n    configs = [\n        {\n            \"path\": \"/kaggle/input/ferplus\",\n            \"has_subset\": True,\n            \"validate\": True\n        },\n        {\n            \"path\": \"/kaggle/input/young-affectnet-hq\",\n            \"has_subset\": False,\n            \"label_map\": EMOTION_MAP,\n            \"validate\": True\n        }\n    ]\n    \n    # Load & merge\n    train_df, val_df = merge_datasets(configs, test_size=0.10, random_state=42)\n\n    \n    # Dataset & transforms\n\n    train_transform, val_transform = get_transforms(img_size=112, use_pretrained=True)\n    \n    train_dataset = EmotionDataset(train_df, transform=train_transform, preload=False)\n    \n    val_dataset = EmotionDataset(val_df, transform=val_transform, preload=False)\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=512,\n        sampler = None,\n        shuffle = True,\n        num_workers=4,\n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=512,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True\n    )\n    \n    \n    \n    print(f\"\\nTrain: {len(train_dataset)} | Val: {len(val_dataset)}\")\n    class_weights = train_dataset.get_class_weights()\n    print(f\"Class weights: {class_weights}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Kiểm tra mapping nhãn\nprint(train_df['label'].value_counts())\n\n# Kiểm tra trùng ảnh giữa train/val\nprint(len(set(train_df['image_path']) & set(val_df['image_path'])))\n\n# Kiểm tra mapping label->idx\nprint(train_dataset.label_to_idx)\n\n# Kiểm tra shape ảnh đầu vào\nimgs, _ = next(iter(train_loader))\nprint(imgs.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_distribution(df, title):\n    counts = df['label'].value_counts().sort_index()\n    plt.figure(figsize=(8,4))\n    counts.plot(kind='bar', color='skyblue', edgecolor='black')\n    plt.title(title)\n    plt.xlabel(\"Emotion label\")\n    plt.ylabel(\"Count\")\n    plt.grid(axis='y', linestyle='--', alpha=0.6)\n    plt.show()\n\nplot_distribution(train_df, \"Train Set Class Distribution\")\nplot_distribution(val_df, \"Validation Set Class Distribution\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ===================================\n# OPTIMIZED SE BLOCK\n# ===================================\nclass SEBlock(nn.Module):\n    \"\"\"\n    Squeeze-and-Excitation Block tối ưu cho inference\n    Sử dụng Linear thay vì Conv2d để giảm overhead\n    \"\"\"\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channels, channels // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        # Squeeze\n        y = self.avg_pool(x).view(b, c)\n        # Excitation\n        y = self.fc(y).view(b, c, 1, 1)\n        # Scale\n        return x * y\n\n\n# ===================================\n# BASIC RESIDUAL BLOCK WITH SE\n# ===================================\nclass BasicBlockSE(nn.Module):\n    \"\"\"\n    Basic Residual Block with SE và dropout được tối ưu\n    \"\"\"\n    expansion = 1\n    \n    def __init__(self, in_channels, out_channels, stride=1, reduction=16, drop_prob=0.0):\n        super().__init__()\n        \n        # Main path\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        # SE attention\n        self.se = SEBlock(out_channels, reduction)\n        \n        # Dropout - đặt SAU conv2, TRƯỚC SE\n        self.drop_prob = drop_prob\n        \n        # Shortcut connection\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = self.shortcut(x)\n        \n        # Conv1 + BN + ReLU\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        \n        # Conv2 + BN\n        out = self.bn2(self.conv2(out))\n        \n        # Dropout (chỉ trong training)\n        if self.training and self.drop_prob > 0:\n            out = F.dropout2d(out, p=self.drop_prob, training=True)\n        \n        # SE attention\n        out = self.se(out)\n        \n        # Residual connection + ReLU\n        out += identity\n        out = F.relu(out, inplace=True)\n        \n        return out\n\nfrom torchvision.models import resnet18\nimport torch.nn as nn\n\nclass EmotionResNet18_SE(nn.Module):\n    def __init__(self, num_classes=8, reduction=8, dropout=0.3, drop_block=0.05, pretrained=True):\n        super().__init__()\n        \n        # Load pretrained ResNet18\n        base_model = resnet18(weights=\"IMAGENET1K_V1\" if pretrained else None)\n        print(\"Loaded pretrained ResNet18 backbone (RGB, ImageNet)\")\n        \n        # Lấy toàn bộ feature layers\n        self.features = nn.Sequential(*list(base_model.children())[:-2])\n        \n        # Chèn SE block ở tầng cuối (tăng focus vùng khuôn mặt)\n        self.se = SEBlock(512, reduction=reduction)\n        \n        # Global pooling + classifier\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.se(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nmodel = EmotionResNet18_SE()\nx = torch.randn(1, 3, 112, 112)\nwith torch.no_grad():\n    x = model.features(x)\n    print(\"features:\", x.shape)\n\n    x = model.se(x)\n    print(\"se:\", x.shape)\n\n    x = model.avgpool(x)\n    print(\"avgpool:\", x.shape)\n\n    x = torch.flatten(x, 1)\n    print(\"flatten:\", x.shape)\n\n    x = model.classifier(x)\n    print(\"classifier:\", x.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.amp as amp\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\n\nimport torch.nn.functional as F\n\n\n# ---------------------------\n# Metrics\n# ---------------------------\ndef compute_metrics(y_true, y_pred, labels=None):\n    acc = accuracy_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    return acc, cm\n\n\n# ---------------------------\n# Training & Evaluation\n# ---------------------------\ndef train_and_evaluate(\n                       out_dir=\"checkpoints\",\n                       img_size=48,\n                       batch_size=256,\n                       epochs=30,\n                       lr=1e-3,\n                       weight_decay=1e-4,\n                       num_workers=4,\n                       device=None,\n                       label_smoothing=0.05,\n                       early_stop_patience=20):\n    \n    os.makedirs(out_dir, exist_ok=True)\n    ckpt_path = os.path.join(out_dir, \"best_emotion_resnet18_se.pth\")\n    final_ckpt_path = os.path.join(out_dir, \"final_emotion_resnet18_se.pth\")\n    history_path = os.path.join(out_dir, \"history.json\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n    device_type = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    if device.type == \"cuda\":\n        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n        print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\n\n    # === Load transform \n    train_t, val_t = get_transforms(img_size)\n\n    # === Model & optimizer\n    model = EmotionResNet18_SE(num_classes=len(train_dataset.label_to_idx))\n    # --- Multi-GPU support ---\n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel\")\n        model = nn.DataParallel(model)\n\n    model = model.to(device)\n        \n    class_weights = train_dataset.get_class_weights().to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing, weight=class_weights)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    warmup_epochs = max(1, int(epochs * 0.05))\n    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n    cosine = CosineAnnealingLR(optimizer, T_max=max(1, epochs - warmup_epochs))\n    scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n    scaler = amp.GradScaler(enabled=(device.type == \"cuda\"))\n\n    # === Resume nếu checkpoint tồn tại\n    best_val, start_epoch, best_epoch, no_improve = 0.0, 0, -1, 0\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n\n    if os.path.exists('/kaggle/input/resnet-emotion/pytorch/default/1/best_emotion_resnet18_se.pth'):\n        # ckpt = torch.load(ckpt_path, map_location=device)\n        ckpt = torch.load('/kaggle/input/resnet-emotion/pytorch/default/1/best_emotion_resnet18_se.pth', map_location=device)\n        model.load_state_dict(ckpt[\"model_state\"])\n        if \"optimizer_state\" in ckpt:\n            optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n        if \"scheduler_state\" in ckpt:\n            scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n        print(f\"[INFO] Loaded existing model from {ckpt_path}\")\n\n        if os.path.exists(\"/kaggle/input/resnet-emotion/pytorch/default/1/history.json\"):\n            with open(\"/kaggle/input/resnet-emotion/pytorch/default/1/history.json\", \"r\") as f:\n                history = json.load(f)\n            best_val = max(history[\"val_acc\"]) / 100\n            start_epoch = len(history[\"val_acc\"])\n            best_epoch = start_epoch - 1\n            print(f\"[INFO]Resuming training from epoch {start_epoch}, best val acc = {best_val*100:.2f}%\")\n\n    label_list = [train_dataset.idx_to_label[i] for i in range(len(train_dataset.idx_to_label))]\n\n    if start_epoch < 5:\n        freeze_epochs = 5\n        for name, param in model.named_parameters():\n            if \"classifier\" not in name:\n                param.requires_grad = False\n        print(f\"[INFO] Freezing backbone for first {freeze_epochs} epochs\")\n        \n        unfreeze_done = False\n\n    # ---------------- TRAINING LOOP ----------------\n    for epoch in range(start_epoch, epochs):\n\n        if start_epoch < 5:\n            # === Unfreeze backbone sau freeze_epochs ===\n            if epoch == freeze_epochs and not unfreeze_done:\n                print(f\"[INFO] Unfreezing backbone at epoch {epoch+1} for fine-tuning\")\n                for param in model.parameters():\n                    param.requires_grad = True\n                unfreeze_done = True\n\n        model.train()\n        running_loss = 0.0\n        y_true_train, y_pred_train = [], []\n        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\", leave=False)\n\n        for imgs, labels in pbar:\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            \n            # with amp.autocast(device_type=device_type, enabled=(device.type==device_type)):\n            with amp.autocast(device_type=\"cuda\", enabled=(device.type==\"cuda\")):\n\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item() * imgs.size(0)\n            preds = outputs.argmax(dim=1).detach().cpu().numpy()\n            y_pred_train.extend(preds.tolist())\n            y_true_train.extend(labels.detach().cpu().numpy().tolist())\n\n        train_loss = running_loss / len(train_dataset)\n        train_acc, _ = compute_metrics(y_true_train, y_pred_train)\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc * 100)\n\n        # Validation\n        model.eval()\n        running_val_loss = 0.0\n        y_true, y_pred = [], []\n        with torch.no_grad(), amp.autocast(device_type=\"cuda\", enabled=(device.type==\"cuda\")):\n            for imgs, labels in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}/{epochs}\", leave=False):\n                imgs, labels = imgs.to(device), labels.to(device)\n                outputs = model(imgs)\n                loss = criterion(outputs, labels)\n                running_val_loss += loss.item() * imgs.size(0)\n                preds = outputs.argmax(dim=1).detach().cpu().numpy()\n                y_pred.extend(preds.tolist())\n                y_true.extend(labels.detach().cpu().numpy().tolist())\n\n        val_loss = running_val_loss / len(val_dataset)\n        val_acc, cm = compute_metrics(y_true, y_pred, labels=list(range(len(train_dataset.idx_to_label))))\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc * 100)\n\n        if epoch % 5 == 0:\n            # --- Log chi tiết để phát hiện thiên vị ---\n            print(\"Class distribution check:\")\n            pred_counts = {label_list[i]: (np.array(y_pred) == i).sum() for i in range(len(label_list))}\n            true_counts = {label_list[i]: (np.array(y_true) == i).sum() for i in range(len(label_list))}\n            for lbl in label_list:\n                ratio = pred_counts[lbl] / max(1, true_counts[lbl])\n                print(f\"  {lbl:<15} pred={pred_counts[lbl]:<6} true={true_counts[lbl]:<6} ratio={ratio:.2f}x\")\n            \n            # --- Classification report ---\n            report = classification_report(y_true, y_pred, target_names=label_list, digits=3)\n            print(\"\\nPer-class performance:\")\n            print(report)\n\n        scheduler.step()\n\n        current_lr = scheduler.get_last_lr()[0]\n\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"LR: {current_lr:.2e}\")\n        print(\"=\" * 70)\n        \n        print(\"Results:\")\n        print(f\"  Train: loss={train_loss:.4f}, acc={train_acc*100:.2f}%\")\n        print(f\"  Val:   loss={val_loss:.4f}, acc={val_acc*100:.2f}%\")\n        print()\n\n        # Checkpoint\n        if val_acc > best_val + 1e-6:\n            best_val = val_acc\n            best_epoch = epoch\n            ckpt = {\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scheduler_state\": scheduler.state_dict(),\n                \"label_to_idx\": train_dataset.label_to_idx\n            }\n            torch.save(ckpt, ckpt_path)\n            print(f\"[INFO] Saved best model (val_acc={val_acc*100:.2f}%)\")\n            no_improve = 0\n        else:\n            no_improve += 1\n            if no_improve >= early_stop_patience:\n                print(f\"[INFO] Early stopping at epoch {epoch+1}, best epoch {best_epoch+1}\")\n                break\n\n        # Lưu history mỗi epoch\n        with open(history_path, \"w\") as f:\n            json.dump(history, f, indent=2)\n\n        if epoch == epochs:\n            # --- Save final state ---\n            torch.save({\n                \"epoch\": epoch,\n                \"model_state\": model.state_dict(),\n                \"optimizer_state\": optimizer.state_dict(),\n                \"scheduler_state\": scheduler.state_dict(),\n                \"label_to_idx\": train_dataset.label_to_idx\n            }, final_ckpt_path)\n\n    # --- Plot ---\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1)\n    plt.plot(history[\"train_loss\"], label=\"train_loss\")\n    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n    plt.legend(); plt.title(\"Loss\")\n    plt.subplot(1,2,2)\n    plt.plot(history[\"train_acc\"], label=\"train_acc\")\n    plt.plot(history[\"val_acc\"], label=\"val_acc\")\n    plt.legend(); plt.title(\"Accuracy\")\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"Best val acc: {best_val*100:.2f}% at epoch {best_epoch+1}\")\n    return model, history, train_dataset.idx_to_label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    # print(\"Labels:\", sorted(train_df['label'].unique()))\n    \n    model, history, idx2label = train_and_evaluate(\n                                                   out_dir=\"checkpoints\",\n                                                   img_size=112, # ver1  =48\n                                                   batch_size=512,\n                                                   epochs=200,\n                                                   lr=3e-4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kkk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if os.path.exists(\"/kaggle/working/checkpoints/history.json\"):\n    with open(\"/kaggle/working/checkpoints/history.json\", \"r\") as f:\n        history = json.load(f)\n    best_val = max(history[\"val_acc\"]) / 100\n    start_epoch = len(history[\"val_acc\"])\n    best_epoch = start_epoch - 1\n\n# --- Plot ---\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(history[\"train_loss\"], label=\"train_loss\")\nplt.plot(history[\"val_loss\"], label=\"val_loss\")\nplt.legend(); plt.title(\"Loss\")\nplt.subplot(1,2,2)\nplt.plot(history[\"train_acc\"], label=\"train_acc\")\nplt.plot(history[\"val_acc\"], label=\"val_acc\")\nplt.legend(); plt.title(\"Accuracy\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image, UnidentifiedImageError\nimport requests\nfrom io import BytesIO\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ndef predict_from_url(model, url, transform, idx_to_label, device=None, show_image=True, top_k=None):\n    \"\"\"\n    Dự đoán cảm xúc từ URL ảnh.\n    Hiển thị top xác suất và biểu đồ cho toàn bộ lớp.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n\n    try:\n        # --- Gửi request ---\n        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n\n        # --- Kiểm tra dữ liệu hợp lệ ---\n        content_type = response.headers.get(\"Content-Type\", \"\")\n        if \"image\" not in content_type:\n            raise ValueError(f\"URL không trả về ảnh. Content-Type: {content_type}\")\n\n        # --- Mở ảnh ---\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n    except (UnidentifiedImageError, ValueError) as e:\n        print(f\"[ERROR] Không thể mở ảnh từ URL: {url}\\nLý do: {e}\")\n        return None, None\n    except Exception as e:\n        print(f\"[ERROR] Lỗi khi tải ảnh: {e}\")\n        return None, None\n\n    # --- Hiển thị ảnh ---\n    if show_image:\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.title(\"Input Image\")\n        plt.show()\n\n    # --- Tiền xử lý ---\n    img_tensor = transform(image).unsqueeze(0).to(device)\n\n    # --- Dự đoán ---\n    with torch.no_grad():\n        outputs = model(img_tensor)\n        probs = F.softmax(outputs, dim=1).cpu().numpy()[0]\n\n    # --- Xử lý kết quả ---\n    emotions = [idx_to_label[i] for i in range(len(probs))]\n    sorted_idx = probs.argsort()[::-1]  # sắp giảm dần\n    top_k = top_k or len(emotions)\n\n    print(\"Top dự đoán:\")\n    for i in range(top_k):\n        lbl = emotions[sorted_idx[i]]\n        conf = probs[sorted_idx[i]] * 100\n        print(f\"  {i+1}. {lbl:10s} : {conf:.2f}%\")\n\n    # --- Biểu đồ xác suất ---\n    plt.figure(figsize=(8, 4))\n    plt.barh([emotions[i] for i in sorted_idx[::-1]],\n             [probs[i]*100 for i in sorted_idx[::-1]],\n             color=\"skyblue\")\n    plt.xlabel(\"Probability (%)\")\n    plt.title(\"Emotion Probabilities\")\n    plt.tight_layout()\n    plt.show()\n\n    # --- Trả về nhãn cao nhất ---\n    pred_label = emotions[sorted_idx[0]]\n    pred_conf = probs[sorted_idx[0]]\n    return pred_label, pred_conf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef load_model_for_inference(ckpt_path, device=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n    ckpt = torch.load(ckpt_path, map_location=device)\n    label_to_idx = ckpt[\"label_to_idx\"]\n    idx_to_label = {v: k for k, v in label_to_idx.items()}\n\n    model = EmotionResNet18_SE(num_classes=len(label_to_idx))\n    model.load_state_dict(ckpt[\"model_state\"])\n    model.to(device)\n    model.eval()\n\n    print(f\"[INFO]Model loaded for inference from: {ckpt_path}\")\n    return model, idx_to_label\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Load model\n    model, idx_to_label = load_model_for_inference(\"/kaggle/working/checkpoints/best_emotion_resnet18_se.pth\", device)\n    # model, idx_to_label = load_model_for_inference(\"/kaggle/input/resnet-emotion/pytorch/default/1/best_emotion_resnet18_se.pth\", device)\n\n    # Get transform (phải giống lúc train)\n    _, val_t = get_transforms(img_size=112)\n\n    # Link ảnh online\n    url = \"https://t4.ftcdn.net/jpg/00/68/69/59/360_F_68695981_GuWIHWfB0l5wJ2al8rv4xZRUqUtwIo2P.jpg\"\n\n    # Dự đoán\n    predict_from_url(model, url, val_t, idx_to_label, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/checkpoints\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/checkpoints.zip /kaggle/working/checkpoints\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\n    \ndownload_file('/kaggle/working/checkpoints', 'checkpoints') ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}